{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f34650",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e489d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c82281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge numpy pandas matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = np.load('data/features_pca.npz')\n",
    "features_pca = data_pca['features_pca']\n",
    "movieIds_pca = data_pca['movieIds_pca']\n",
    "\n",
    "data_svd = np.load('data/features_svd.npz')\n",
    "features_svd = data_svd['features_svd']\n",
    "movieIds_svd = data_svd['movieIds_svd']\n",
    "\n",
    "data_lda = np.load('data/features_lda.npz')\n",
    "features_lda = data_lda['features_lda']\n",
    "movieIds_lda = data_lda['movieIds_lda']\n",
    "\n",
    "movies = pd.read_csv('data/train_complete.csv')\n",
    "movies = pd.concat([movies, pd.read_csv('data/test_complete.csv')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad22e08",
   "metadata": {},
   "source": [
    "## Selected Algorithm\n",
    "\n",
    "1. K-means (Partitioning method)\n",
    "2. Gaussian Mixture Model - GMM (Distriburion-based method)\n",
    "\n",
    "### K-means\n",
    "\n",
    "- Industry standard for image clustering\n",
    "- Fast and scalable for large datasers\n",
    "- Works well when clusters are spherical and similar in size\n",
    "- Easy to interpret: each movie belongs to exactly one cluster\n",
    "- Suitable for poster features where visual styles form distinct groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab825544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=10, max_iters=100, tol=1e-4, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.centroids = self._kmeans_plus_plus(X)\n",
    "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
    "        self.inertia_ = 0.0\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            distances = cdist(X, self.centroids, metric='euclidean')\n",
    "            self.labels_ = np.argmin(distances, axis=1)\n",
    "            \n",
    "            new_centroids = np.array([X[self.labels_ == k].mean(axis=0) \n",
    "                                     for k in range(self.n_clusters)])\n",
    "            \n",
    "            if np.allclose(self.centroids, new_centroids, atol=self.tol):\n",
    "                print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "                \n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        self.inertia_ = np.sum((X - self.centroids[self.labels_])**2)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _kmeans_plus_plus(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        centroids = []\n",
    "        \n",
    "        first_idx = np.random.randint(n_samples)\n",
    "        centroids.append(X[first_idx])\n",
    "        \n",
    "        for _ in range(1, self.n_clusters):\n",
    "            distances = cdist(X, np.array(centroids), metric='euclidean')\n",
    "            min_distances = np.min(distances, axis=1)\n",
    "            probabilities = min_distances ** 2\n",
    "            probabilities /= probabilities.sum()\n",
    "            \n",
    "            next_idx = np.random.choice(n_samples, p=probabilities)\n",
    "            centroids.append(X[next_idx])\n",
    "        \n",
    "        return np.array(centroids)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = cdist(X, self.centroids, metric='euclidean')\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2104cfd",
   "metadata": {},
   "source": [
    "### GMM\n",
    "\n",
    "- Probabilistic assignments: captures uncertainty in cluster memebership\n",
    "- Flexible cluster shapes: can model elliptical clusters\n",
    "- Better for overlapping styles: a poster can have mixed characteristics\n",
    "- Natural for movie posters: genres often blend (action-comedy, scifi-fi-drama, etc)\n",
    "- Provides probability scores useful for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel:\n",
    "    def __init__(self, n_components=10, max_iters=100, tol=1e-4, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means_ = X[indices]\n",
    "        \n",
    "        self.covariances_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "        \n",
    "        log_likelihood_old = 0\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            responsibilities = self._e_step(X)\n",
    "            self._m_step(X, responsibilities)\n",
    "            log_likelihood = self._compute_log_likelihood(X)\n",
    "            \n",
    "            if abs(log_likelihood - log_likelihood_old) < self.tol:\n",
    "                print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "                \n",
    "            log_likelihood_old = log_likelihood\n",
    "        \n",
    "        self.labels_ = self.predict(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                responsibilities[:, k] = self.weights_[k] * multivariate_normal.pdf(\n",
    "                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True\n",
    "                )\n",
    "            except:\n",
    "                responsibilities[:, k] = 1e-10\n",
    "        \n",
    "        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n",
    "        responsibilities /= (responsibilities_sum + 1e-10)\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        Nk = responsibilities.sum(axis=0)\n",
    "        self.weights_ = Nk / n_samples\n",
    "        \n",
    "        self.means_ = (responsibilities.T @ X) / Nk[:, np.newaxis]\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means_[k]\n",
    "            weighted_diff = responsibilities[:, k][:, np.newaxis] * diff\n",
    "            self.covariances_[k] = (weighted_diff.T @ diff) / Nk[k]\n",
    "            \n",
    "            self.covariances_[k] += np.eye(n_features) * 1e-6\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        log_likelihood = 0\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                log_likelihood += np.sum(\n",
    "                    np.log(self.weights_[k] * multivariate_normal.pdf(\n",
    "                        X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True\n",
    "                    ) + 1e-10)\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return log_likelihood / n_samples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        responsibilities = self._e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self._e_step(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

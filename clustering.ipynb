{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f34650",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e489d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c82281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge numpy pandas matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9368db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import silhouette_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a73a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = np.load('data/features_pca.npz')\n",
    "features_pca = data_pca['features']\n",
    "movieIds_pca = data_pca['movieId']\n",
    "\n",
    "data_svd = np.load('data/features_svd.npz')\n",
    "features_svd = data_svd['features']\n",
    "movieIds_svd = data_svd['movieId']\n",
    "\n",
    "data_lda = np.load('data/features_lda.npz')\n",
    "features_lda = data_lda['features']\n",
    "movieIds_lda = data_lda['movieId']\n",
    "\n",
    "movies = pd.read_csv('data/train_complete.csv')\n",
    "movies = pd.concat([movies, pd.read_csv('data/test_complete.csv')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad22e08",
   "metadata": {},
   "source": [
    "## Selected Algorithms\n",
    "\n",
    "1. K-means (Partitioning method)\n",
    "2. Gaussian Mixture Model - GMM (Distriburion-based method)\n",
    "\n",
    "### K-means\n",
    "\n",
    "- Industry standard for image clustering\n",
    "- Fast and scalable for large datasers\n",
    "- Works well when clusters are spherical and similar in size\n",
    "- Easy to interpret: each movie belongs to exactly one cluster\n",
    "- Suitable for poster features where visual styles form distinct groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab825544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=10, max_iters=100, tol=1e-4, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.centroids = self._kmeans_plus_plus(X)\n",
    "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
    "        self.inertia_ = 0.0\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            distances = cdist(X, self.centroids, metric='euclidean')\n",
    "            self.labels_ = np.argmin(distances, axis=1)\n",
    "            \n",
    "            new_centroids = np.array([X[self.labels_ == k].mean(axis=0) \n",
    "                                     for k in range(self.n_clusters)])\n",
    "            \n",
    "            if np.allclose(self.centroids, new_centroids, atol=self.tol):\n",
    "                break\n",
    "                \n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        self.inertia_ = np.sum((X - self.centroids[self.labels_])**2)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _kmeans_plus_plus(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        centroids = []\n",
    "        \n",
    "        first_idx = np.random.randint(n_samples)\n",
    "        centroids.append(X[first_idx])\n",
    "        \n",
    "        for _ in range(1, self.n_clusters):\n",
    "            distances = cdist(X, np.array(centroids), metric='euclidean')\n",
    "            min_distances = np.min(distances, axis=1)\n",
    "            probabilities = min_distances ** 2\n",
    "            probabilities /= probabilities.sum()\n",
    "            \n",
    "            next_idx = np.random.choice(n_samples, p=probabilities)\n",
    "            centroids.append(X[next_idx])\n",
    "        \n",
    "        return np.array(centroids)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = cdist(X, self.centroids, metric='euclidean')\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2104cfd",
   "metadata": {},
   "source": [
    "### GMM\n",
    "\n",
    "- Probabilistic assignments: captures uncertainty in cluster memebership\n",
    "- Flexible cluster shapes: can model elliptical clusters\n",
    "- Better for overlapping styles: a poster can have mixed characteristics\n",
    "- Natural for movie posters: genres often blend (action-comedy, scifi-fi-drama, etc)\n",
    "- Provides probability scores useful for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f8ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel:\n",
    "    def __init__(self, n_components=10, max_iters=100, tol=1e-4, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means_ = X[indices].copy()\n",
    "        self.covariances_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
    "        \n",
    "        log_likelihood_old = 0\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            responsibilities = self._e_step(X)\n",
    "            self._m_step(X, responsibilities)\n",
    "            log_likelihood = self._compute_log_likelihood(X)\n",
    "            \n",
    "            if abs(log_likelihood - log_likelihood_old) < self.tol:\n",
    "                break\n",
    "                \n",
    "            log_likelihood_old = log_likelihood\n",
    "        \n",
    "        self.labels_ = self.predict(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                responsibilities[:, k] = self.weights_[k] * multivariate_normal.pdf(\n",
    "                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True\n",
    "                )\n",
    "            except:\n",
    "                responsibilities[:, k] = 1e-10\n",
    "        \n",
    "        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n",
    "        responsibilities /= (responsibilities_sum + 1e-10)\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        Nk = responsibilities.sum(axis=0)\n",
    "        self.weights_ = Nk / n_samples\n",
    "        \n",
    "        self.means_ = (responsibilities.T @ X) / Nk[:, np.newaxis]\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means_[k]\n",
    "            weighted_diff = responsibilities[:, k][:, np.newaxis] * diff\n",
    "            self.covariances_[k] = (weighted_diff.T @ diff) / Nk[k]\n",
    "            \n",
    "            self.covariances_[k] += np.eye(n_features) * 1e-6\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        log_likelihood = 0\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                log_likelihood += np.sum(\n",
    "                    np.log(self.weights_[k] * multivariate_normal.pdf(\n",
    "                        X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True\n",
    "                    ) + 1e-10)\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return log_likelihood / n_samples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        responsibilities = self._e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self._e_step(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d1486",
   "metadata": {},
   "source": [
    "## Optimal Number of Clusters\n",
    "\n",
    "### Hint from number of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872df000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique genres in dataset: 19\n",
      "Genres: ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n"
     ]
    }
   ],
   "source": [
    "all_genres = set()\n",
    "\n",
    "for genre_str in movies['genres'].dropna():\n",
    "    all_genres.update(genre_str.split('|'))\n",
    "\n",
    "n_genres = len(all_genres)\n",
    "\n",
    "print(f\"Number of unique genres in dataset: {n_genres}\")\n",
    "print(f\"Genres: {sorted(all_genres)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for k=9\n",
      "\tPCA: 0.0088\n",
      "\tSVD: 0.0163\n",
      "\tLDA: 0.0743\n",
      "\tAvg: 0.0332\n",
      "Evaluating for k=10\n",
      "\tPCA: 0.0093\n",
      "\tSVD: 0.0142\n",
      "\tLDA: 0.0816\n",
      "\tAvg: 0.0351\n",
      "Evaluating for k=11\n",
      "\tPCA: 0.0124\n",
      "\tSVD: 0.0104\n",
      "\tLDA: 0.0888\n",
      "\tAvg: 0.0372\n",
      "Evaluating for k=12\n",
      "\tPCA: 0.0106\n",
      "\tSVD: 0.0117\n",
      "\tLDA: 0.0981\n",
      "\tAvg: 0.0401\n",
      "Evaluating for k=13\n",
      "\tPCA: 0.0107\n",
      "\tSVD: 0.0125\n",
      "\tLDA: 0.0995\n",
      "\tAvg: 0.0409\n",
      "Evaluating for k=14\n",
      "\tPCA: 0.0111\n",
      "\tSVD: 0.0110\n",
      "\tLDA: 0.1036\n",
      "\tAvg: 0.0419\n",
      "Evaluating for k=15\n",
      "\tPCA: 0.0115\n",
      "\tSVD: 0.0107\n",
      "\tLDA: 0.1100\n",
      "\tAvg: 0.0441\n",
      "Evaluating for k=16\n",
      "\tPCA: 0.0114\n",
      "\tSVD: 0.0110\n",
      "\tLDA: 0.0953\n",
      "\tAvg: 0.0392\n",
      "Evaluating for k=17\n",
      "\tPCA: 0.0126\n",
      "\tSVD: 0.0107\n",
      "\tLDA: 0.0844\n",
      "\tAvg: 0.0359\n",
      "Evaluating for k=18\n",
      "\tPCA: 0.0132\n",
      "\tSVD: 0.0109\n",
      "\tLDA: 0.0792\n",
      "\tAvg: 0.0344\n",
      "\n",
      "Optimal number of clusters for K-means based on average silhouette score: 15\n"
     ]
    }
   ],
   "source": [
    "n_range = range(max(n_genres // 2, 1), n_genres, 1)\n",
    "\n",
    "reduction_methods = {\n",
    "    'PCA': (features_pca, movieIds_pca),\n",
    "    'SVD': (features_svd, movieIds_svd),\n",
    "    'LDA': (features_lda, movieIds_lda)\n",
    "}\n",
    "\n",
    "avg_silhouettes = []\n",
    "\n",
    "for n in n_range:\n",
    "    print(f\"Evaluating for k={n}\")\n",
    "    k_silhouettes = []\n",
    "\n",
    "    for method, (features, movieIds) in reduction_methods.items():\n",
    "        kmeans = KMeans(n_clusters=n, max_iters=50)\n",
    "        labels = kmeans.fit_predict(features)\n",
    "        sil = silhouette_score(features, labels)\n",
    "        k_silhouettes.append(sil)\n",
    "        print(f\"\\t{method}: {sil:.4f}\")\n",
    "    \n",
    "    avg = np.mean(k_silhouettes)\n",
    "    avg_silhouettes.append(avg)\n",
    "    print(f\"\\tAvg: {avg:.4f}\")\n",
    "\n",
    "optimal_n_kmeans = list(n_range)[np.argmax(avg_silhouettes)]\n",
    "print(f\"\\nOptimal number of clusters for K-means based on average silhouette score: {optimal_n_kmeans}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

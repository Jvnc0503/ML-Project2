{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f34650",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e489d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c82281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge numpy pandas matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9368db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import silhouette_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a73a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = np.load('data/features_pca.npz')\n",
    "features_pca = data_pca['features']\n",
    "movieIds_pca = data_pca['movieId']\n",
    "\n",
    "data_svd = np.load('data/features_svd.npz')\n",
    "features_svd = data_svd['features']\n",
    "movieIds_svd = data_svd['movieId']\n",
    "\n",
    "data_lda = np.load('data/features_lda.npz')\n",
    "features_lda = data_lda['features']\n",
    "movieIds_lda = data_lda['movieId']\n",
    "\n",
    "movies = pd.read_csv('data/train_complete.csv')\n",
    "movies = pd.concat([movies, pd.read_csv('data/test_complete.csv')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad22e08",
   "metadata": {},
   "source": [
    "## Selected Algorithms\n",
    "\n",
    "1. K-means (Partitioning method)\n",
    "2. Gaussian Mixture Model - GMM (Distriburion-based method)\n",
    "\n",
    "### K-means\n",
    "\n",
    "- Industry standard for image clustering\n",
    "- Fast and scalable for large datasers\n",
    "- Works well when clusters are spherical and similar in size\n",
    "- Easy to interpret: each movie belongs to exactly one cluster\n",
    "- Suitable for poster features where visual styles form distinct groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab825544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=10, max_iters=100, tol=1e-4, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.centroids = self._kmeans_plus_plus(X)\n",
    "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
    "        self.inertia_ = 0.0\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            distances = cdist(X, self.centroids, metric='euclidean')\n",
    "            self.labels_ = np.argmin(distances, axis=1)\n",
    "            \n",
    "            new_centroids = np.array([X[self.labels_ == k].mean(axis=0) \n",
    "                                     for k in range(self.n_clusters)])\n",
    "            \n",
    "            if np.allclose(self.centroids, new_centroids, atol=self.tol):\n",
    "                break\n",
    "                \n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        self.inertia_ = np.sum((X - self.centroids[self.labels_])**2)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _kmeans_plus_plus(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        centroids = []\n",
    "        \n",
    "        first_idx = np.random.randint(n_samples)\n",
    "        centroids.append(X[first_idx])\n",
    "        \n",
    "        for _ in range(1, self.n_clusters):\n",
    "            distances = cdist(X, np.array(centroids), metric='euclidean')\n",
    "            min_distances = np.min(distances, axis=1)\n",
    "            probabilities = min_distances ** 2\n",
    "            probabilities /= probabilities.sum()\n",
    "            \n",
    "            next_idx = np.random.choice(n_samples, p=probabilities)\n",
    "            centroids.append(X[next_idx])\n",
    "        \n",
    "        return np.array(centroids)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = cdist(X, self.centroids, metric='euclidean')\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2104cfd",
   "metadata": {},
   "source": [
    "### GMM\n",
    "\n",
    "- Probabilistic assignments: captures uncertainty in cluster memebership\n",
    "- Flexible cluster shapes: can model elliptical clusters\n",
    "- Better for overlapping styles: a poster can have mixed characteristics\n",
    "- Natural for movie posters: genres often blend (action-comedy, scifi-fi-drama, etc)\n",
    "- Provides probability scores useful for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f8ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel:\n",
    "    def __init__(self, n_components=10, max_iters=100, tol=1e-4, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means_ = X[indices].copy()\n",
    "        self.covariances_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
    "        \n",
    "        log_likelihood_old = 0\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            responsibilities = self._e_step(X)\n",
    "            self._m_step(X, responsibilities)\n",
    "            log_likelihood = self._compute_log_likelihood(X)\n",
    "            \n",
    "            if abs(log_likelihood - log_likelihood_old) < self.tol:\n",
    "                break\n",
    "                \n",
    "            log_likelihood_old = log_likelihood\n",
    "        \n",
    "        self.labels_ = self.predict(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                responsibilities[:, k] = self.weights_[k] * multivariate_normal.pdf(\n",
    "                    X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True\n",
    "                )\n",
    "            except:\n",
    "                responsibilities[:, k] = 1e-10\n",
    "        \n",
    "        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n",
    "        responsibilities /= (responsibilities_sum + 1e-10)\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        Nk = responsibilities.sum(axis=0)\n",
    "        self.weights_ = Nk / n_samples\n",
    "        \n",
    "        self.means_ = (responsibilities.T @ X) / Nk[:, np.newaxis]\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means_[k]\n",
    "            weighted_diff = responsibilities[:, k][:, np.newaxis] * diff\n",
    "            self.covariances_[k] = (weighted_diff.T @ diff) / Nk[k]\n",
    "            \n",
    "            self.covariances_[k] += np.eye(n_features) * 1e-6\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        log_likelihood = 0\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                log_likelihood += np.sum(\n",
    "                    np.log(self.weights_[k] * multivariate_normal.pdf(\n",
    "                        X, mean=self.means_[k], cov=self.covariances_[k], allow_singular=True\n",
    "                    ) + 1e-10)\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return log_likelihood / n_samples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        responsibilities = self._e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self._e_step(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d1486",
   "metadata": {},
   "source": [
    "## Optimal Number of Clusters\n",
    "\n",
    "### Hint from number of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872df000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique genres in dataset: 19\n",
      "Genres: ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n"
     ]
    }
   ],
   "source": [
    "all_genres = set()\n",
    "\n",
    "for genre_str in movies['genres'].dropna():\n",
    "    all_genres.update(genre_str.split('|'))\n",
    "\n",
    "n_genres = len(all_genres)\n",
    "\n",
    "print(f\"Number of unique genres in dataset: {n_genres}\")\n",
    "print(f\"Genres: {sorted(all_genres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fdc67",
   "metadata": {},
   "source": [
    "### Optimal number for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c3f1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for n=9\n",
      "    PCA: 0.0088\n",
      "    SVD: 0.0163\n",
      "    LDA: 0.0743\n",
      "Avg: 0.0332\n",
      "\n",
      "Evaluating for n=10\n",
      "    PCA: 0.0093\n",
      "    SVD: 0.0142\n",
      "    LDA: 0.0816\n",
      "Avg: 0.0351\n",
      "\n",
      "Evaluating for n=11\n",
      "    PCA: 0.0124\n",
      "    SVD: 0.0104\n",
      "    LDA: 0.0888\n",
      "Avg: 0.0372\n",
      "\n",
      "Evaluating for n=12\n",
      "    PCA: 0.0106\n",
      "    SVD: 0.0117\n",
      "    LDA: 0.0981\n",
      "Avg: 0.0401\n",
      "\n",
      "Evaluating for n=13\n",
      "    PCA: 0.0107\n",
      "    SVD: 0.0125\n",
      "    LDA: 0.0995\n",
      "Avg: 0.0409\n",
      "\n",
      "Evaluating for n=14\n",
      "    PCA: 0.0111\n",
      "    SVD: 0.0110\n",
      "    LDA: 0.1036\n",
      "Avg: 0.0419\n",
      "\n",
      "Evaluating for n=15\n",
      "    PCA: 0.0115\n",
      "    SVD: 0.0107\n",
      "    LDA: 0.1100\n",
      "Avg: 0.0441\n",
      "\n",
      "Evaluating for n=16\n",
      "    PCA: 0.0114\n",
      "    SVD: 0.0110\n",
      "    LDA: 0.0953\n",
      "Avg: 0.0392\n",
      "\n",
      "Evaluating for n=17\n",
      "    PCA: 0.0126\n",
      "    SVD: 0.0107\n",
      "    LDA: 0.0844\n",
      "Avg: 0.0359\n",
      "\n",
      "Evaluating for n=18\n",
      "    PCA: 0.0132\n",
      "    SVD: 0.0109\n",
      "    LDA: 0.0792\n",
      "Avg: 0.0344\n",
      "\n",
      "Evaluating for n=19\n",
      "    PCA: 0.0123\n",
      "    SVD: 0.0101\n",
      "    LDA: 0.0808\n",
      "Avg: 0.0344\n",
      "\n",
      "Optimal number of clusters for K-means based on average silhouette score: 15\n"
     ]
    }
   ],
   "source": [
    "n_range = range(max(n_genres // 2, 2), n_genres + 1, 1)\n",
    "\n",
    "reduction_methods = {\n",
    "    'PCA': (features_pca, movieIds_pca),\n",
    "    'SVD': (features_svd, movieIds_svd),\n",
    "    'LDA': (features_lda, movieIds_lda)\n",
    "}\n",
    "\n",
    "avg_silhouettes = []\n",
    "\n",
    "for n in n_range:\n",
    "    print(f\"Evaluating for n={n}\")\n",
    "    n_silhouettes = []\n",
    "\n",
    "    for method, (features, movieIds) in reduction_methods.items():\n",
    "        kmeans = KMeans(n_clusters=n, max_iters=50)\n",
    "        labels = kmeans.fit_predict(features)\n",
    "        sil = silhouette_score(features, labels)\n",
    "        n_silhouettes.append(sil)\n",
    "        print(f\"    {method}: {sil:.4f}\")\n",
    "    \n",
    "    avg = np.mean(n_silhouettes)\n",
    "    avg_silhouettes.append(np.mean(n_silhouettes))\n",
    "    print(f\"Avg: {avg:.4f}\\n\")\n",
    "\n",
    "optimal_n_kmeans = list(n_range)[np.argmax(avg_silhouettes)]\n",
    "print(f\"Optimal number of clusters for K-means based on average silhouette score: {optimal_n_kmeans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5f46e",
   "metadata": {},
   "source": [
    "### Optimal number for GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db289ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for n=9\n",
      "    PCA: BIC=3873318.05, AIC=3865033.63\n",
      "    SVD: BIC=3873318.05, AIC=3865033.63\n",
      "    LDA: BIC=3815555.98, AIC=3813571.05\n",
      "Avg BIC:3854064.03, Avg AIC:3847879.44\n",
      "\n",
      "Evaluating for n=10\n",
      "    PCA: BIC=4303687.74, AIC=4294482.04\n",
      "    SVD: BIC=4303687.74, AIC=4294482.04\n",
      "    LDA: BIC=4244370.72, AIC=4242164.46\n",
      "Avg BIC:4283915.40, Avg AIC:4277042.84\n",
      "\n",
      "Evaluating for n=11\n",
      "    PCA: BIC=4734057.43, AIC=4723930.44\n",
      "    SVD: BIC=4734057.43, AIC=4723930.44\n",
      "    LDA: BIC=4673967.99, AIC=4671540.38\n",
      "Avg BIC:4714027.62, Avg AIC:4706467.09\n",
      "\n",
      "Evaluating for n=12\n",
      "    PCA: BIC=5164427.11, AIC=5153378.84\n",
      "    SVD: BIC=5164427.11, AIC=5153378.84\n",
      "    LDA: BIC=5101129.75, AIC=5098480.80\n",
      "Avg BIC:5143327.99, Avg AIC:5135079.49\n",
      "\n",
      "Evaluating for n=13\n",
      "    PCA: BIC=5594796.80, AIC=5582827.25\n",
      "    SVD: BIC=5594796.80, AIC=5582827.25\n",
      "    LDA: BIC=5530415.06, AIC=5527544.77\n",
      "Avg BIC:5573336.22, Avg AIC:5564399.75\n",
      "\n",
      "Evaluating for n=14\n",
      "    PCA: BIC=6025166.49, AIC=6012275.65\n",
      "    SVD: BIC=6025166.49, AIC=6012275.65\n",
      "    LDA: BIC=5959745.75, AIC=5956654.12\n",
      "Avg BIC:6003359.58, Avg AIC:5993735.14\n",
      "\n",
      "Evaluating for n=15\n",
      "    PCA: BIC=6455536.18, AIC=6441724.05\n",
      "    SVD: BIC=6455536.18, AIC=6441724.05\n",
      "    LDA: BIC=6388883.50, AIC=6385570.53\n",
      "Avg BIC:6433318.62, Avg AIC:6423006.21\n",
      "\n",
      "Evaluating for n=16\n",
      "    PCA: BIC=6885905.87, AIC=6871172.46\n",
      "    SVD: BIC=6885905.87, AIC=6871172.46\n",
      "    LDA: BIC=6817909.60, AIC=6814375.29\n",
      "Avg BIC:6863240.44, Avg AIC:6852240.07\n",
      "\n",
      "Evaluating for n=17\n",
      "    PCA: BIC=7316275.55, AIC=7300620.86\n",
      "    SVD: BIC=7316275.55, AIC=7300620.86\n",
      "    LDA: BIC=7246405.21, AIC=7242649.56\n",
      "Avg BIC:7292985.44, Avg AIC:7281297.09\n",
      "\n",
      "Evaluating for n=18\n",
      "    PCA: BIC=7746645.24, AIC=7730069.26\n",
      "    SVD: BIC=7746645.24, AIC=7730069.26\n",
      "    LDA: BIC=7676325.59, AIC=7672348.59\n",
      "Avg BIC:7723205.36, Avg AIC:7710829.04\n",
      "\n",
      "Evaluating for n=19\n",
      "    PCA: BIC=8177014.93, AIC=8159517.67\n",
      "    SVD: BIC=8177014.93, AIC=8159517.67\n",
      "    LDA: BIC=8105585.44, AIC=8101387.10\n",
      "Avg BIC:8153205.10, Avg AIC:8140140.81\n",
      "\n",
      "Optimal number of clusters for GMM:\n",
      "    Based on BIC: 9\n",
      "    Based on AIC: 9\n",
      "(Both criteria agree on 9 clusters)\n"
     ]
    }
   ],
   "source": [
    "n_range = range(max(n_genres // 2, 1), n_genres + 1, 1)\n",
    "\n",
    "reduction_methods = {\n",
    "    'PCA': (features_pca, movieIds_pca),\n",
    "    'SVD': (features_svd, movieIds_svd),\n",
    "    'LDA': (features_lda, movieIds_lda)\n",
    "}\n",
    "\n",
    "avg_bics = []\n",
    "avg_aics = []\n",
    "\n",
    "for n in n_range:\n",
    "    print(f\"Evaluating for n={n}\")\n",
    "    n_bics = []\n",
    "    n_aics = []\n",
    "\n",
    "    for method, (features, movieIds) in reduction_methods.items():\n",
    "        gmm = GaussianMixtureModel(n_components=n, max_iters=50)\n",
    "        gmm.fit(features)\n",
    "\n",
    "        log_likelihood = gmm._compute_log_likelihood(features) * features.shape[0]\n",
    "        n_params = n * features.shape[1] * 2 + n - 1\n",
    "\n",
    "        bic = -2 * log_likelihood + n_params * np.log(features.shape[0])\n",
    "        aic = -2 * log_likelihood + 2 * n_params\n",
    "\n",
    "        n_bics.append(bic)\n",
    "        n_aics.append(aic)\n",
    "        print(f\"    {method}: BIC={bic:.2f}, AIC={aic:.2f}\")\n",
    "    \n",
    "    avg_bic = np.mean(n_bics)\n",
    "    avg_aic = np.mean(n_aics)\n",
    "    avg_bics.append(avg_bic)\n",
    "    avg_aics.append(avg_aic)\n",
    "    print(f\"Avg BIC:{avg_bic:.2f}, Avg AIC:{avg_aic:.2f}\\n\")\n",
    "\n",
    "optimal_n_gmm_bic = list(n_range)[np.argmin(avg_bics)]\n",
    "optimal_n_gmm_aic = list(n_range)[np.argmin(avg_aics)]\n",
    "\n",
    "print(\"Optimal number of clusters for GMM:\")\n",
    "print(f\"    Based on BIC: {optimal_n_gmm_bic}\")\n",
    "print(f\"    Based on AIC: {optimal_n_gmm_aic}\")\n",
    "if optimal_n_gmm_bic == optimal_n_gmm_aic:\n",
    "    optimal_n_gmm = optimal_n_gmm_bic\n",
    "    print(f\"(Both criteria agree on {optimal_n_gmm_bic} clusters)\")\n",
    "else:\n",
    "    optimal_n_gmm = (optimal_n_gmm_aic + optimal_n_gmm_bic) // 2\n",
    "    print(f\"(Choosing average: {optimal_n_gmm} clusters)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
